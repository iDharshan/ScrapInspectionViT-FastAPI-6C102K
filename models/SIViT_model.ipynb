{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Fine-Tuning ViT with LoRA for Steel Scrap Inspection**\n",
    "This notebook fine-tunes a Vision Transformer (ViT) model using LoRA (Low-Rank Adaptation) to classify scrap metal images into six categories based on their physical properties. The dataset is stored locally (15GB), and weâ€™ll optimize training for limited hardware (16GB RAM, 8GB VRAM) with class weights to handle potential imbalances.\n",
    "\n",
    "## **Steel Scrap Classes**\n",
    "Weâ€™re classifying scrap metal into the following categories:\n",
    "\n",
    "* E1: Old thin steel scrap (â‰¤1.5x0.5x0.5 m, thickness <6 mm)\n",
    "* E2: Thick new production steel scrap (â‰¤1.5x0.5x0.5 m, thickness â‰¥3 mm)\n",
    "* E3: Old thick steel scrap (â‰¤1.5x0.5x0.5 m, thickness â‰¥6 mm)\n",
    "* E6: Thin new production steel scrap, compressed or baled (thickness <3 mm)\n",
    "* E8: Thin new production steel scrap (â‰¤1.5x0.5x0.5 m, thickness <3 mm)\n",
    "* EHRB: Old and new steel scrap, mainly rebars and merchant bars (max 1.5x0.5x0.5 m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1. Import Libraries**\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "* Imports handle dataset management (datasets), model training (transformers), image processing (torchvision, PIL), LoRA adaptation (peft), and class weight computation (sklearn)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "from datasets import Dataset, Image as HFImage\n",
    "from transformers import AutoImageProcessor, AutoModelForImageClassification, TrainingArguments, Trainer\n",
    "from torchvision.transforms import (\n",
    "    CenterCrop,\n",
    "    Compose,\n",
    "    Normalize,\n",
    "    RandomHorizontalFlip,\n",
    "    RandomResizedCrop,\n",
    "    Resize,\n",
    "    ToTensor,\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model\n",
    "import evaluate\n",
    "from sklearn.utils.class_weight import compute_class_weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2. Load the Dataset**\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "* Loads our 15GB dataset from D:\\LLM\\DOES\\does_dataset_images.\n",
    "* Each folder (e.g., E8) represents a scrap metal class. Images are paired with their file paths and labels.\n",
    "* HFImage() marks the \"image\" column as image data for efficient processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 102399 examples\n"
     ]
    }
   ],
   "source": [
    "# Define dataset path and class folders\n",
    "output_dir = r\"D:\\LLM\\DOES\\does_dataset_images\"\n",
    "folders = [\"E8\", \"E3\", \"E1\", \"E2\", \"E6\", \"EHRB\"]\n",
    "\n",
    "def create_dataset():\n",
    "    data = []\n",
    "    for folder in folders:\n",
    "        folder_path = os.path.join(output_dir, folder)\n",
    "        for img_file in os.listdir(folder_path):\n",
    "            if os.path.isfile(os.path.join(folder_path, img_file)):\n",
    "                data.append({\"image\": os.path.join(folder_path, img_file), \"label\": folder})\n",
    "    return Dataset.from_list(data).cast_column(\"image\", HFImage())\n",
    "\n",
    "dataset = create_dataset()\n",
    "print(f\"Dataset size: {len(dataset)} examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3. Define Label Mappings and Class Weights**\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "* **Mappings**: label2id and id2label convert between class names (e.g., E8) and IDs (0-5).\n",
    "* **Class Meanings:** Defines what each class representsâ€”useful for interpreting predictions.\n",
    "* **Class Weights:** Uses 'balanced' mode to give higher weights to underrepresented classes (e.g., if EHRB has fewer images than E8). This balances training focus, improving accuracy on rare classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class weights: [0.93586861 1.6772973  0.80297826 1.93038118 0.43210705 3.88139641]\n"
     ]
    }
   ],
   "source": [
    "# Label mappings\n",
    "labels = folders\n",
    "label2id = {label: i for i, label in enumerate(labels)}\n",
    "id2label = {i: label for i, label in enumerate(labels)}\n",
    "\n",
    "# Class meanings for reference\n",
    "class_meanings = {\n",
    "    \"E1\": \"Old thin steel scrap (â‰¤1.5x0.5x0.5 m, thickness <6 mm)\",\n",
    "    \"E2\": \"Thick new production steel scrap (â‰¤1.5x0.5x0.5 m, thickness â‰¥3 mm)\",\n",
    "    \"E3\": \"Old thick steel scrap (â‰¤1.5x0.5x0.5 m, thickness â‰¥6 mm)\",\n",
    "    \"E6\": \"Thin new production steel scrap, compressed or baled (thickness <3 mm)\",\n",
    "    \"E8\": \"Thin new production steel scrap (â‰¤1.5x0.5x0.5 m, thickness <3 mm)\",\n",
    "    \"EHRB\": \"Old and new steel scrap, mainly rebars and merchant bars (max 1.5x0.5x0.5 m)\"\n",
    "}\n",
    "\n",
    "# Compute class weights for imbalance\n",
    "label_list = [example[\"label\"] for example in dataset]\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(label_list),\n",
    "    y=label_list\n",
    ")\n",
    "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float)\n",
    "print(f\"Class weights: {class_weights}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4. Set Up Image Preprocessing**\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "* **Image Processor:** Ensures images match ViTâ€™s expected format (224x224, normalized).\n",
    "* **Transforms:**\n",
    "    * **Training:** Adds randomness (RandomResizedCrop, RandomHorizontalFlip) to make the model generalize better.\n",
    "    * **Validation:** Uses fixed resizing for consistent evaluation.\n",
    "* **Split:** 20% validation set (increased from 10%) provides a bigger sample to test performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n"
     ]
    }
   ],
   "source": [
    "# Load image processor\n",
    "model_checkpoint = \"google/vit-base-patch16-224\"\n",
    "image_processor = AutoImageProcessor.from_pretrained(model_checkpoint)\n",
    "\n",
    "# Define transforms\n",
    "normalize = Normalize(mean=image_processor.image_mean, std=image_processor.image_std)\n",
    "train_transforms = Compose([\n",
    "    RandomResizedCrop(image_processor.size[\"height\"]),  # Random crop\n",
    "    RandomHorizontalFlip(),                            # Augmentation: random flip\n",
    "    ToTensor(),                                        # Convert to tensor\n",
    "    normalize                                          # Normalize for ViT\n",
    "])\n",
    "val_transforms = Compose([\n",
    "    Resize(image_processor.size[\"height\"]),            # Consistent resize\n",
    "    CenterCrop(image_processor.size[\"height\"]),        # Center crop\n",
    "    ToTensor(),\n",
    "    normalize\n",
    "])\n",
    "\n",
    "# Preprocessing functions\n",
    "def preprocess_train(example_batch):\n",
    "    images = [Image.open(img).convert(\"RGB\") if isinstance(img, str) else img.convert(\"RGB\") \n",
    "              for img in example_batch[\"image\"]]\n",
    "    example_batch[\"pixel_values\"] = [train_transforms(img) for img in images]\n",
    "    example_batch[\"labels\"] = [label2id[label] for label in example_batch[\"label\"]]\n",
    "    return example_batch\n",
    "\n",
    "def preprocess_val(example_batch):\n",
    "    images = [Image.open(img).convert(\"RGB\") if isinstance(img, str) else img.convert(\"RGB\") \n",
    "              for img in example_batch[\"image\"]]\n",
    "    example_batch[\"pixel_values\"] = [val_transforms(img) for img in images]\n",
    "    example_batch[\"labels\"] = [label2id[label] for label in example_batch[\"label\"]]\n",
    "    return example_batch\n",
    "\n",
    "# Split dataset (80% train, 20% validation)\n",
    "splits = dataset.train_test_split(test_size=0.2)\n",
    "train_ds = splits[\"train\"]\n",
    "val_ds = splits[\"test\"]\n",
    "train_ds.set_transform(preprocess_train)\n",
    "val_ds.set_transform(preprocess_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **5. Load Model with LoRA**\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "* **LoRA:** Adapts only a small part of ViT, keeping memory usage low (fits 8GB VRAM).\n",
    "* **Hyperparameters:**\n",
    "    * <span style=\"color: orange;\">r=16:</span> Higher rank (from 8) increases capacity for learning scrap metal features.\n",
    "    * <span style=\"color: orange;\">lora_alpha=16:</span> Matches r for balanced adaptation strength.\n",
    "    * <span style=\"color: orange;\">lora_dropout=0.3:</span> Higher dropout (from 0.1) reduces overfitting on your dataset.\n",
    "    * <span style=\"color: orange\">target_modules:</span> Focuses on attention layers (query, value), critical for ViTâ€™s performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([6]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([1000, 768]) in the checkpoint and torch.Size([6, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load base ViT model\n",
    "model = AutoModelForImageClassification.from_pretrained(\n",
    "    model_checkpoint,\n",
    "    label2id=label2id,\n",
    "    id2label=id2label,\n",
    "    ignore_mismatched_sizes=True,\n",
    ")\n",
    "\n",
    "# Configure LoRA\n",
    "config = LoraConfig(\n",
    "    r=16,              # LoRA rank\n",
    "    lora_alpha=16,     # Scaling factor\n",
    "    target_modules=[\"query\", \"value\"],  # ViT attention layers\n",
    "    lora_dropout=0.3,  # Dropout rate\n",
    "    bias=\"none\",       # No bias adaptation\n",
    "    modules_to_save=[\"classifier\"],  # Save classifier weights\n",
    ")\n",
    "lora_model = get_peft_model(model, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **6. Define Metrics and Collate Function**\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "* **Metrics:** Measures accuracyâ€”straightforward for classification.\n",
    "* **Collate:** Combines images and labels into batches for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load accuracy metric\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions = np.argmax(eval_pred.predictions, axis=1)\n",
    "    return {\"accuracy\": metric.compute(predictions=predictions, references=eval_pred.label_ids)[\"accuracy\"]}\n",
    "\n",
    "# Collate function for batching\n",
    "def collate_fn(examples):\n",
    "    pixel_values = torch.stack([example[\"pixel_values\"] for example in examples])\n",
    "    labels = torch.tensor([example[\"labels\"] for example in examples])\n",
    "    return {\"pixel_values\": pixel_values, \"labels\": labels}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **7. Custom Trainer with Class Weights**\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "* Customizes the Trainer to apply class weights in the loss function.\n",
    "* **Why:** Our dataset has a significant class imbalance, meaning some classes have way more images than others. Without weights, the model would focus too much on frequent classes (e.g., *E8*) and ignore rare ones (e.g., *EHRB*), tanking accuracy on underrepresented categories. Class weights penalize mistakes on rare classes more heavily, balancing the modelâ€™s attention across all scrap metal types.\n",
    "\n",
    "**Class Distribution**\n",
    "Hereâ€™s the number of images per class in the dataset:\n",
    "\n",
    "* **E8:** 39,496 (most common)\n",
    "* **E3:** 21,254\n",
    "* **E1:** 18,236\n",
    "* **E2:** 10,175\n",
    "* **E6:** 8,841\n",
    "* **EHRB:** 4,397 (least common)\n",
    "\n",
    "**Total Images:** 102,399\n",
    "\n",
    "**Imbalance:** *E8* has ~9x more samples than *EHRB*, skewing the model toward *E8* without correction.\n",
    "\n",
    "**Class Weights and Penalties**\n",
    "We use *compute_class_weight('balanced')* to assign weights inversely proportional to class frequency. The formula is:\n",
    "Weight for class \\( i \\):\n",
    "$$\n",
    "\\text{Weight for class } i = \\frac{\\text{total samples}}{\\text{number of classes} \\cdot \\text{samples in class } i}\n",
    "$$\n",
    "\n",
    "**Example Calculation:**\n",
    "* Total samples = 102,399  \n",
    "* Number of classes = 6  \n",
    "\n",
    "**Weights per Class:**\n",
    "\n",
    "* **E8**: `0.432` â€“ (Lowest penalty, most common)  \n",
    "* **E3**: `0.803`  \n",
    "* **E1**: `0.936`  \n",
    "* **E2**: `1.677`  \n",
    "* **E6**: `1.931`  \n",
    "* **EHRB**: `3.879` â€“ (Highest penalty, rarest) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightedTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        labels = inputs.get(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "        loss_fct = torch.nn.CrossEntropyLoss(weight=class_weights_tensor.to(logits.device))\n",
    "        loss = loss_fct(logits, labels)\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **8. Set Training Arguments**\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "* Hyperparameters:\n",
    "    * <span style='color: orange;'>learning_rate=2e-4:</span> Reduced (from 5e-3) for precise updates, avoiding overshooting.\n",
    "    * <span style='color: orange;'>batch_size=8, gradient_accumulation_steps=4:</span> Effective batch size of 32 fits 8GB VRAM with FP16.\n",
    "    * <span style='color: orange;'>num_train_epochs=5:</span> More epochs (from 3) for better convergence, with early stopping via load_best_model_at_end.\n",
    "    * <span style='color: orange;'>weight_decay=0.01:</span> Adds regularization to prevent overfitting.\n",
    "    * <span style='color: orange'>warmup_ratio=0.1:</span> Gradually ramps up learning rate over 10% of steps for stability.\n",
    "**Why:** Optimizes for your hardware and dataset, balancing speed and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dharshan\\anaconda3\\envs\\llmpt\\lib\\site-packages\\transformers\\training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "batch_size = 32\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=f\"{model_name}-finetuned-lora-metalscrap\",\n",
    "    remove_unused_columns=False,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-4,              # Lowered learning rate\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    fp16=True,                       # Mixed precision\n",
    "    num_train_epochs=5,              # More epochs\n",
    "    logging_steps=10,                # Frequent logging\n",
    "    load_best_model_at_end=True,     # Keep best model\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    push_to_hub=False,\n",
    "    label_names=[\"labels\"],\n",
    "    weight_decay=0.01,               # Regularization\n",
    "    warmup_ratio=0.1,                # Warmup period\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **9. Train the Model**\n",
    "\n",
    "**What Happens:**\n",
    "\n",
    "* Trains for 5 epochs, evaluating and saving after each.\n",
    "* Applies class weights to balance loss across classes.\n",
    "* Logs loss every 10 stepsâ€”expect it to decrease steadily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dharshan\\AppData\\Local\\Temp\\ipykernel_25156\\2778547215.py:2: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = WeightedTrainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='12800' max='12800' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [12800/12800 1:48:47, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.093600</td>\n",
       "      <td>0.049235</td>\n",
       "      <td>0.985840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.090200</td>\n",
       "      <td>0.030784</td>\n",
       "      <td>0.989990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.040400</td>\n",
       "      <td>0.019799</td>\n",
       "      <td>0.992822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.025900</td>\n",
       "      <td>0.016324</td>\n",
       "      <td>0.994971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.015900</td>\n",
       "      <td>0.014208</td>\n",
       "      <td>0.995654</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dharshan\\anaconda3\\envs\\llmpt\\lib\\site-packages\\peft\\utils\\other.py:716: UserWarning: Unable to fetch remote file due to the following error (ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 4b0306a2-ff24-42f3-a53f-b577c0636016)') - silently ignoring the lookup for the file config.json in google/vit-base-patch16-224.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Dharshan\\anaconda3\\envs\\llmpt\\lib\\site-packages\\peft\\utils\\save_and_load.py:246: UserWarning: Could not find a config file in google/vit-base-patch16-224 - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Dharshan\\anaconda3\\envs\\llmpt\\lib\\site-packages\\peft\\utils\\other.py:716: UserWarning: Unable to fetch remote file due to the following error (ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: d093b3fa-e55d-4104-be3d-a11149c01d1c)') - silently ignoring the lookup for the file config.json in google/vit-base-patch16-224.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Dharshan\\anaconda3\\envs\\llmpt\\lib\\site-packages\\peft\\utils\\save_and_load.py:246: UserWarning: Could not find a config file in google/vit-base-patch16-224 - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Dharshan\\anaconda3\\envs\\llmpt\\lib\\site-packages\\peft\\utils\\other.py:716: UserWarning: Unable to fetch remote file due to the following error (ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: ae6f22a0-1636-4eed-8278-1e04709b0265)') - silently ignoring the lookup for the file config.json in google/vit-base-patch16-224.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Dharshan\\anaconda3\\envs\\llmpt\\lib\\site-packages\\peft\\utils\\save_and_load.py:246: UserWarning: Could not find a config file in google/vit-base-patch16-224 - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=12800, training_loss=0.11891007877886295, metrics={'train_runtime': 6530.1723, 'train_samples_per_second': 62.723, 'train_steps_per_second': 1.96, 'total_flos': 3.1961371690685768e+19, 'train_loss': 0.11891007877886295, 'epoch': 5.0})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize trainer\n",
    "trainer = WeightedTrainer(\n",
    "    model=lora_model,\n",
    "    args=args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    tokenizer=image_processor,\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=collate_fn,\n",
    ")\n",
    "\n",
    "# Train\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1c929b9188e44af8a0c59d8c3a1548c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/2.38M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fafe9dba56e48db82d52dabe5f7824f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/5.17k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dharshan\\anaconda3\\envs\\llmpt\\lib\\site-packages\\huggingface_hub\\file_download.py:142: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Dharshan\\.cache\\huggingface\\hub\\models--iDharshan--vit-base-patch16-224-SIViT. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model pushed to: https://huggingface.co/iDharshan/vit-base-patch16-224-SIViT\n"
     ]
    }
   ],
   "source": [
    "repo_name = f\"iDharshan/{model_name}-SIViT\"\n",
    "lora_model.push_to_hub(repo_name)\n",
    "image_processor.push_to_hub(repo_name)\n",
    "print(f\"Model pushed to: https://huggingface.co/{repo_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
