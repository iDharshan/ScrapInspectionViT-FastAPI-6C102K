{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Fine-Tuning ViT with LoRA for Steel Scrap Inspection**\n",
    "This notebook fine-tunes a Vision Transformer (ViT) model using LoRA (Low-Rank Adaptation) to classify scrap metal images into six categories based on their physical properties. The dataset is stored locally (15GB), and we’ll optimize training for limited hardware (16GB RAM, 8GB VRAM) with class weights to handle potential imbalances.\n",
    "\n",
    "## **Steel Scrap Classes**\n",
    "We’re classifying scrap metal into the following categories:\n",
    "\n",
    "* E1: Old thin steel scrap (≤1.5x0.5x0.5 m, thickness <6 mm)\n",
    "* E2: Thick new production steel scrap (≤1.5x0.5x0.5 m, thickness ≥3 mm)\n",
    "* E3: Old thick steel scrap (≤1.5x0.5x0.5 m, thickness ≥6 mm)\n",
    "* E6: Thin new production steel scrap, compressed or baled (thickness <3 mm)\n",
    "* E8: Thin new production steel scrap (≤1.5x0.5x0.5 m, thickness <3 mm)\n",
    "* EHRB: Old and new steel scrap, mainly rebars and merchant bars (max 1.5x0.5x0.5 m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1. Import Libraries**\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "* Imports handle dataset management (datasets), model training (transformers), image processing (torchvision, PIL), LoRA adaptation (peft), and class weight computation (sklearn)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "from datasets import Dataset, Image as HFImage\n",
    "from transformers import AutoImageProcessor, AutoModelForImageClassification, TrainingArguments, Trainer\n",
    "from torchvision.transforms import (\n",
    "    CenterCrop,\n",
    "    Compose,\n",
    "    Normalize,\n",
    "    RandomHorizontalFlip,\n",
    "    RandomResizedCrop,\n",
    "    Resize,\n",
    "    ToTensor,\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model\n",
    "import evaluate\n",
    "from sklearn.utils.class_weight import compute_class_weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2. Load the Dataset**\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "* Loads our 15GB dataset from D:\\LLM\\DOES\\does_dataset_images.\n",
    "* Each folder (e.g., E8) represents a scrap metal class. Images are paired with their file paths and labels.\n",
    "* HFImage() marks the \"image\" column as image data for efficient processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 102399 examples\n"
     ]
    }
   ],
   "source": [
    "# Define dataset path and class folders\n",
    "output_dir = r\"D:\\LLM\\DOES\\does_dataset_images\"\n",
    "folders = [\"E8\", \"E3\", \"E1\", \"E2\", \"E6\", \"EHRB\"]\n",
    "\n",
    "def create_dataset():\n",
    "    data = []\n",
    "    for folder in folders:\n",
    "        folder_path = os.path.join(output_dir, folder)\n",
    "        for img_file in os.listdir(folder_path):\n",
    "            if os.path.isfile(os.path.join(folder_path, img_file)):\n",
    "                data.append({\"image\": os.path.join(folder_path, img_file), \"label\": folder})\n",
    "    return Dataset.from_list(data).cast_column(\"image\", HFImage())\n",
    "\n",
    "dataset = create_dataset()\n",
    "print(f\"Dataset size: {len(dataset)} examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3. Define Label Mappings and Class Weights**\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "* **Mappings**: label2id and id2label convert between class names (e.g., E8) and IDs (0-5).\n",
    "* **Class Meanings:** Defines what each class represents—useful for interpreting predictions.\n",
    "* **Class Weights:** Uses 'balanced' mode to give higher weights to underrepresented classes (e.g., if EHRB has fewer images than E8). This balances training focus, improving accuracy on rare classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class weights: [0.93586861 1.6772973  0.80297826 1.93038118 0.43210705 3.88139641]\n"
     ]
    }
   ],
   "source": [
    "# Label mappings\n",
    "labels = folders\n",
    "label2id = {label: i for i, label in enumerate(labels)}\n",
    "id2label = {i: label for i, label in enumerate(labels)}\n",
    "\n",
    "# Class meanings for reference\n",
    "class_meanings = {\n",
    "    \"E1\": \"Old thin steel scrap (≤1.5x0.5x0.5 m, thickness <6 mm)\",\n",
    "    \"E2\": \"Thick new production steel scrap (≤1.5x0.5x0.5 m, thickness ≥3 mm)\",\n",
    "    \"E3\": \"Old thick steel scrap (≤1.5x0.5x0.5 m, thickness ≥6 mm)\",\n",
    "    \"E6\": \"Thin new production steel scrap, compressed or baled (thickness <3 mm)\",\n",
    "    \"E8\": \"Thin new production steel scrap (≤1.5x0.5x0.5 m, thickness <3 mm)\",\n",
    "    \"EHRB\": \"Old and new steel scrap, mainly rebars and merchant bars (max 1.5x0.5x0.5 m)\"\n",
    "}\n",
    "\n",
    "# Compute class weights for imbalance\n",
    "label_list = [example[\"label\"] for example in dataset]\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(label_list),\n",
    "    y=label_list\n",
    ")\n",
    "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float)\n",
    "print(f\"Class weights: {class_weights}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4. Set Up Image Preprocessing**\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "* **Image Processor:** Ensures images match ViT’s expected format (224x224, normalized).\n",
    "* **Transforms:**\n",
    "    * **Training:** Adds randomness (RandomResizedCrop, RandomHorizontalFlip) to make the model generalize better.\n",
    "    * **Validation:** Uses fixed resizing for consistent evaluation.\n",
    "* **Split:** 20% validation set (increased from 10%) provides a bigger sample to test performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n"
     ]
    }
   ],
   "source": [
    "# Load image processor\n",
    "model_checkpoint = \"google/vit-base-patch16-224\"\n",
    "image_processor = AutoImageProcessor.from_pretrained(model_checkpoint)\n",
    "\n",
    "# Define transforms\n",
    "normalize = Normalize(mean=image_processor.image_mean, std=image_processor.image_std)\n",
    "train_transforms = Compose([\n",
    "    RandomResizedCrop(image_processor.size[\"height\"]),  # Random crop\n",
    "    RandomHorizontalFlip(),                            # Augmentation: random flip\n",
    "    ToTensor(),                                        # Convert to tensor\n",
    "    normalize                                          # Normalize for ViT\n",
    "])\n",
    "val_transforms = Compose([\n",
    "    Resize(image_processor.size[\"height\"]),            # Consistent resize\n",
    "    CenterCrop(image_processor.size[\"height\"]),        # Center crop\n",
    "    ToTensor(),\n",
    "    normalize\n",
    "])\n",
    "\n",
    "# Preprocessing functions\n",
    "def preprocess_train(example_batch):\n",
    "    images = [Image.open(img).convert(\"RGB\") if isinstance(img, str) else img.convert(\"RGB\") \n",
    "              for img in example_batch[\"image\"]]\n",
    "    example_batch[\"pixel_values\"] = [train_transforms(img) for img in images]\n",
    "    example_batch[\"labels\"] = [label2id[label] for label in example_batch[\"label\"]]\n",
    "    return example_batch\n",
    "\n",
    "def preprocess_val(example_batch):\n",
    "    images = [Image.open(img).convert(\"RGB\") if isinstance(img, str) else img.convert(\"RGB\") \n",
    "              for img in example_batch[\"image\"]]\n",
    "    example_batch[\"pixel_values\"] = [val_transforms(img) for img in images]\n",
    "    example_batch[\"labels\"] = [label2id[label] for label in example_batch[\"label\"]]\n",
    "    return example_batch\n",
    "\n",
    "# Split dataset (80% train, 20% validation)\n",
    "splits = dataset.train_test_split(test_size=0.2)\n",
    "train_ds = splits[\"train\"]\n",
    "val_ds = splits[\"test\"]\n",
    "train_ds.set_transform(preprocess_train)\n",
    "val_ds.set_transform(preprocess_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **5. Load Model with LoRA**\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "* **LoRA:** Adapts only a small part of ViT, keeping memory usage low (fits 8GB VRAM).\n",
    "* **Hyperparameters:**\n",
    "    * <span style=\"color: orange;\">r=16:</span> Higher rank (from 8) increases capacity for learning scrap metal features.\n",
    "    * <span style=\"color: orange;\">lora_alpha=16:</span> Matches r for balanced adaptation strength.\n",
    "    * <span style=\"color: orange;\">lora_dropout=0.3:</span> Higher dropout (from 0.1) reduces overfitting on your dataset.\n",
    "    * <span style=\"color: orange\">target_modules:</span> Focuses on attention layers (query, value), critical for ViT’s performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([6]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([1000, 768]) in the checkpoint and torch.Size([6, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load base ViT model\n",
    "model = AutoModelForImageClassification.from_pretrained(\n",
    "    model_checkpoint,\n",
    "    label2id=label2id,\n",
    "    id2label=id2label,\n",
    "    ignore_mismatched_sizes=True,\n",
    ")\n",
    "\n",
    "# Configure LoRA\n",
    "config = LoraConfig(\n",
    "    r=16,              # LoRA rank\n",
    "    lora_alpha=16,     # Scaling factor\n",
    "    target_modules=[\"query\", \"value\"],  # ViT attention layers\n",
    "    lora_dropout=0.3,  # Dropout rate\n",
    "    bias=\"none\",       # No bias adaptation\n",
    "    modules_to_save=[\"classifier\"],  # Save classifier weights\n",
    ")\n",
    "lora_model = get_peft_model(model, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **6. Define Metrics and Collate Function**\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "* **Metrics:** Measures accuracy—straightforward for classification.\n",
    "* **Collate:** Combines images and labels into batches for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load accuracy metric\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions = np.argmax(eval_pred.predictions, axis=1)\n",
    "    return {\"accuracy\": metric.compute(predictions=predictions, references=eval_pred.label_ids)[\"accuracy\"]}\n",
    "\n",
    "# Collate function for batching\n",
    "def collate_fn(examples):\n",
    "    pixel_values = torch.stack([example[\"pixel_values\"] for example in examples])\n",
    "    labels = torch.tensor([example[\"labels\"] for example in examples])\n",
    "    return {\"pixel_values\": pixel_values, \"labels\": labels}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **7. Custom Trainer with Class Weights**\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "* Customizes the Trainer to apply class weights in the loss function.\n",
    "* **Why:** Our dataset has a significant class imbalance, meaning some classes have way more images than others. Without weights, the model would focus too much on frequent classes (e.g., *E8*) and ignore rare ones (e.g., *EHRB*), tanking accuracy on underrepresented categories. Class weights penalize mistakes on rare classes more heavily, balancing the model’s attention across all scrap metal types.\n",
    "\n",
    "**Class Distribution**\n",
    "Here’s the number of images per class in the dataset:\n",
    "\n",
    "* **E8:** 39,496 (most common)\n",
    "* **E3:** 21,254\n",
    "* **E1:** 18,236\n",
    "* **E2:** 10,175\n",
    "* **E6:** 8,841\n",
    "* **EHRB:** 4,397 (least common)\n",
    "\n",
    "**Total Images:** 102,399\n",
    "\n",
    "**Imbalance:** *E8* has ~9x more samples than *EHRB*, skewing the model toward *E8* without correction.\n",
    "\n",
    "**Class Weights and Penalties**\n",
    "We use *compute_class_weight('balanced')* to assign weights inversely proportional to class frequency. The formula is:\n",
    "Weight for class \\( i \\):\n",
    "$$\n",
    "\\text{Weight for class } i = \\frac{\\text{total samples}}{\\text{number of classes} \\cdot \\text{samples in class } i}\n",
    "$$\n",
    "\n",
    "**Example Calculation:**\n",
    "* Total samples = 102,399  \n",
    "* Number of classes = 6  \n",
    "\n",
    "**Weights per Class:**\n",
    "\n",
    "* **E8**: `0.432` – (Lowest penalty, most common)  \n",
    "* **E3**: `0.803`  \n",
    "* **E1**: `0.936`  \n",
    "* **E2**: `1.677`  \n",
    "* **E6**: `1.931`  \n",
    "* **EHRB**: `3.879` – (Highest penalty, rarest) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightedTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        labels = inputs.get(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "        loss_fct = torch.nn.CrossEntropyLoss(weight=class_weights_tensor.to(logits.device))\n",
    "        loss = loss_fct(logits, labels)\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **8. Set Training Arguments**\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "* Hyperparameters:\n",
    "    * <span style='color: orange;'>learning_rate=2e-4:</span> Reduced (from 5e-3) for precise updates, avoiding overshooting.\n",
    "    * <span style='color: orange;'>batch_size=8, gradient_accumulation_steps=4:</span> Effective batch size of 32 fits 8GB VRAM with FP16.\n",
    "    * <span style='color: orange;'>num_train_epochs=5:</span> More epochs (from 3) for better convergence, with early stopping via load_best_model_at_end.\n",
    "    * <span style='color: orange;'>weight_decay=0.01:</span> Adds regularization to prevent overfitting.\n",
    "    * <span style='color: orange'>warmup_ratio=0.1:</span> Gradually ramps up learning rate over 10% of steps for stability.\n",
    "**Why:** Optimizes for your hardware and dataset, balancing speed and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dharshan\\anaconda3\\envs\\llmpt\\lib\\site-packages\\transformers\\training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "batch_size = 32\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=f\"{model_name}-finetuned-lora-metalscrap\",\n",
    "    remove_unused_columns=False,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-4,              # Lowered learning rate\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    fp16=True,                       # Mixed precision\n",
    "    num_train_epochs=5,              # More epochs\n",
    "    logging_steps=10,                # Frequent logging\n",
    "    load_best_model_at_end=True,     # Keep best model\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    push_to_hub=False,\n",
    "    label_names=[\"labels\"],\n",
    "    weight_decay=0.01,               # Regularization\n",
    "    warmup_ratio=0.1,                # Warmup period\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **9. Train the Model**\n",
    "\n",
    "**What Happens:**\n",
    "\n",
    "* Trains for 5 epochs, evaluating and saving after each.\n",
    "* Applies class weights to balance loss across classes.\n",
    "* Logs loss every 10 steps—expect it to decrease steadily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dharshan\\AppData\\Local\\Temp\\ipykernel_25156\\2778547215.py:2: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = WeightedTrainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='12800' max='12800' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [12800/12800 1:48:47, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.093600</td>\n",
       "      <td>0.049235</td>\n",
       "      <td>0.985840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.090200</td>\n",
       "      <td>0.030784</td>\n",
       "      <td>0.989990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.040400</td>\n",
       "      <td>0.019799</td>\n",
       "      <td>0.992822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.025900</td>\n",
       "      <td>0.016324</td>\n",
       "      <td>0.994971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.015900</td>\n",
       "      <td>0.014208</td>\n",
       "      <td>0.995654</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dharshan\\anaconda3\\envs\\llmpt\\lib\\site-packages\\peft\\utils\\other.py:716: UserWarning: Unable to fetch remote file due to the following error (ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 4b0306a2-ff24-42f3-a53f-b577c0636016)') - silently ignoring the lookup for the file config.json in google/vit-base-patch16-224.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Dharshan\\anaconda3\\envs\\llmpt\\lib\\site-packages\\peft\\utils\\save_and_load.py:246: UserWarning: Could not find a config file in google/vit-base-patch16-224 - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Dharshan\\anaconda3\\envs\\llmpt\\lib\\site-packages\\peft\\utils\\other.py:716: UserWarning: Unable to fetch remote file due to the following error (ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: d093b3fa-e55d-4104-be3d-a11149c01d1c)') - silently ignoring the lookup for the file config.json in google/vit-base-patch16-224.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Dharshan\\anaconda3\\envs\\llmpt\\lib\\site-packages\\peft\\utils\\save_and_load.py:246: UserWarning: Could not find a config file in google/vit-base-patch16-224 - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Dharshan\\anaconda3\\envs\\llmpt\\lib\\site-packages\\peft\\utils\\other.py:716: UserWarning: Unable to fetch remote file due to the following error (ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: ae6f22a0-1636-4eed-8278-1e04709b0265)') - silently ignoring the lookup for the file config.json in google/vit-base-patch16-224.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Dharshan\\anaconda3\\envs\\llmpt\\lib\\site-packages\\peft\\utils\\save_and_load.py:246: UserWarning: Could not find a config file in google/vit-base-patch16-224 - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=12800, training_loss=0.11891007877886295, metrics={'train_runtime': 6530.1723, 'train_samples_per_second': 62.723, 'train_steps_per_second': 1.96, 'total_flos': 3.1961371690685768e+19, 'train_loss': 0.11891007877886295, 'epoch': 5.0})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize trainer\n",
    "trainer = WeightedTrainer(\n",
    "    model=lora_model,\n",
    "    args=args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    tokenizer=image_processor,\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=collate_fn,\n",
    ")\n",
    "\n",
    "# Train\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1c929b9188e44af8a0c59d8c3a1548c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/2.38M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fafe9dba56e48db82d52dabe5f7824f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/5.17k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dharshan\\anaconda3\\envs\\llmpt\\lib\\site-packages\\huggingface_hub\\file_download.py:142: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Dharshan\\.cache\\huggingface\\hub\\models--iDharshan--vit-base-patch16-224-SIViT. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model pushed to: https://huggingface.co/iDharshan/vit-base-patch16-224-SIViT\n"
     ]
    }
   ],
   "source": [
    "repo_name = f\"iDharshan/{model_name}-SIViT\"\n",
    "lora_model.push_to_hub(repo_name)\n",
    "image_processor.push_to_hub(repo_name)\n",
    "print(f\"Model pushed to: https://huggingface.co/{repo_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
